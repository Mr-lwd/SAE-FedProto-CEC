
@article{lecun1998mnist,
  title={The MNIST database of handwritten digits},
  author={LeCun, Yann},
  journal={http://yann. lecun. com/exdb/mnist/},
  year={1998}
}

@article{xiao2017fashion,
  title={Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms},
  author={Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  journal={arXiv preprint arXiv:1708.07747},
  year={2017}
}

@article{krizhevsky2009learningcifar10,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}

@inproceedings{zhou_towards_2024,
	title = {Towards {Efficient} {Asynchronous} {Federated} {Learning} in {Heterogeneous} {Edge} {Environments}},
	url = {https://ieeexplore.ieee.org/document/10621333},
	doi = {10.1109/INFOCOM52122.2024.10621333},
	abstract = {Federated learning (FL) is widely used in edge environments as a privacy-preserving collaborative learning paradigm. However, edge devices often have heterogeneous computation capabilities and data distributions, hampering the efficiency of co-training. Existing works develop staleness-aware semi-asynchronous FL that reduces the contribution of slow devices to the global model to mitigate their negative impacts. But this makes data on slow devices unable to be fully leveraged in global model updating, exacerbating the effects of data heterogeneity. In this paper, to cope with both system and data heterogeneity, we propose a clustering and two-stage aggregation-based Efficient Asynchronous Federated Learning (EAFL) framework, which can achieve better learning performance with higher efficiency in heterogeneous edge environments. In EAFL, we first propose a gradient similarity-based dynamic clustering mechanism to cluster devices with similar system and data characteristics together dynamically during the training process. Then, we develop a novel two-stage aggregation strategy consisting of staleness-aware semi-asynchronous intra-cluster aggregation and data size-aware synchronous inter-cluster aggregation to efficiently and comprehensively aggregate training updates across heterogeneous clusters. With that, the negative impacts of slow devices and Non-IID data can be simultaneously alleviated, thus achieving efficient collaborative learning. Extensive experiments demonstrate that EAFL is superior to state-of-the-art methods.},
	urldate = {2025-01-02},
	booktitle = {{IEEE} {INFOCOM} 2024 - {IEEE} {Conference} on {Computer} {Communications}},
	author = {Zhou, Yajie and Pang, Xiaoyi and Wang, Zhibo and Hu, Jiahui and Sun, Peng and Ren, Kui},
	month = may,
	year = {2024},
	note = {ISSN: 2641-9874},
	keywords = {Adaptation models, Aggregates, Asynchronous Federated Learning, Clustering, Collaboration, Computational modeling, Data Heterogeneity, Federated learning, Performance evaluation, System Heterogeneity, Training},
	pages = {2448--2457},
	file = {Full Text PDF:files/231/Zhou 等 - 2024 - Towards Efficient Asynchronous Federated Learning .pdf:application/pdf;IEEE Xplore Abstract Record:files/232/10621333.html:text/html},
}

@book{tan_fedproto_2021,
	title = {{FedProto}: {Federated} {Prototype} {Learning} over {Heterogeneous} {Devices}},
	shorttitle = {{FedProto}},
	abstract = {The heterogeneity across devices usually hinders the optimization convergence and generalization performance of federated learning (FL) when the aggregation of devices' knowledge occurs in the gradient space. For example, devices may differ in terms of data distribution, network latency, input/output space, and/or model architecture, which can easily lead to the misalignment of their local gradients. To improve the tolerance to heterogeneity, we propose a novel federated prototype learning (FedProto) framework in which the devices and server communicate the class prototypes instead of the gradients. FedProto aggregates the local prototypes collected from different devices, and then sends the global prototypes back to all devices to regularize the training of local models. The training on each device aims to minimize the classification error on the local data while keeping the resulting local prototypes sufficiently close to the corresponding global ones. Through experiments, we propose a benchmark setting tailored for heterogeneous FL, with FedProto outperforming several recent FL approaches on multiple datasets.},
	author = {Tan, Yue and Long, Guodong and Liu, Lu and Zhou, Tianyi and Jiang, Jing},
	month = may,
	year = {2021},
	doi = {10.48550/arXiv.2105.00243},
}

@article{zhang_fedtgp_2024,
	title = {{FedTGP}: {Trainable} {Global} {Prototypes} with {Adaptive}-{Margin}-{Enhanced} {Contrastive} {Learning} for {Data} and {Model} {Heterogeneity} in {Federated} {Learning}},
	volume = {38},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{FedTGP}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/29617},
	doi = {10.1609/aaai.v38i15.29617},
	abstract = {Recently, Heterogeneous Federated Learning (HtFL) has attracted attention due to its ability to support heterogeneous models and data. To reduce the high communication cost of transmitting model parameters, a major challenge in HtFL, prototype-based HtFL methods are proposed to solely share class representatives, a.k.a, prototypes, among heterogeneous clients while maintaining the privacy of clients’ models. However, these prototypes are naively aggregated into global prototypes on the server using weighted averaging, resulting in suboptimal global knowledge which negatively impacts the performance of clients. To overcome this challenge, we introduce a novel HtFL approach called FedTGP, which leverages our Adaptive-margin-enhanced Contrastive Learning (ACL) to learn Trainable Global Prototypes (TGP) on the server. By incorporating ACL, our approach enhances prototype separability while preserving semantic meaning. Extensive experiments with twelve heterogeneous models demonstrate that our FedTGP surpasses state-of-the-art methods by up to 9.08\% in accuracy while maintaining the communication and privacy advantages of prototype-based HtFL. Our code is available at https://github.com/TsingZ0/FedTGP.},
	language = {en},
	number = {15},
	urldate = {2025-01-02},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhang, Jianqing and Liu, Yang and Hua, Yang and Cao, Jian},
	month = mar,
	year = {2024},
	note = {Number: 15},
	keywords = {ML: Distributed Machine Learning \& Federated Learning},
	pages = {16768--16776},
	file = {Full Text PDF:files/240/Zhang 等 - 2024 - FedTGP Trainable Global Prototypes with Adaptive-.pdf:application/pdf},
}

@article{liu_hierarchical_2023,
	title = {Hierarchical {Federated} {Learning} {With} {Quantization}: {Convergence} {Analysis} and {System} {Design}},
	volume = {22},
	issn = {1558-2248},
	shorttitle = {Hierarchical {Federated} {Learning} {With} {Quantization}},
	url = {https://ieeexplore.ieee.org/abstract/document/9834296},
	doi = {10.1109/TWC.2022.3190512},
	abstract = {Federated learning (FL) is a powerful distributed machine learning framework where a server aggregates models trained by different clients without accessing their private data. Hierarchical FL, with a client-edge-cloud aggregation hierarchy, can effectively leverage both the cloud server’s access to many clients’ data and the edge servers’ closeness to the clients to achieve a high communication efficiency. Neural network quantization can further reduce the communication overhead during model uploading. To fully exploit the advantages of hierarchical FL, an accurate convergence analysis with respect to the key system parameters is needed. Unfortunately, existing analysis is loose and does not consider model quantization. In this paper, we derive a tighter convergence bound for hierarchical FL with quantization. The convergence result leads to practical guidelines for important design problems such as the client-edge aggregation and edge-client association strategies. Based on the obtained analytical results, we optimize the two aggregation intervals and show that the client-edge aggregation interval should slowly decay while the edge-cloud aggregation interval needs to adapt to the ratio of the client-edge and edge-cloud propagation delay. Simulation results shall verify the design guidelines and demonstrate the effectiveness of the proposed aggregation strategy.},
	number = {1},
	urldate = {2025-01-02},
	journal = {IEEE Transactions on Wireless Communications},
	author = {Liu, Lumin and Zhang, Jun and Song, Shenghui and Letaief, Khaled B.},
	month = jan,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Wireless Communications},
	keywords = {Convergence, convergence analysis, edge learning, Federated learning, Guidelines, Optimization, Quantization (signal), Servers, System analysis and design, Training},
	pages = {2--18},
	file = {已提交版本:files/245/Liu 等 - 2023 - Hierarchical Federated Learning With Quantization.pdf:application/pdf},
}

@inproceedings{liu_client-edge-cloud_2020,
	title = {Client-{Edge}-{Cloud} {Hierarchical} {Federated} {Learning}},
	url = {https://ieeexplore.ieee.org/abstract/document/9148862},
	doi = {10.1109/ICC40277.2020.9148862},
	abstract = {Federated Learning is a collaborative machine learning framework to train a deep learning model without accessing clients’ private data. Previous works assume one central parameter server either at the cloud or at the edge. The cloud server can access more data but with excessive communication overhead and long latency, while the edge server enjoys more efficient communications with the clients. To combine their advantages, we propose a client-edge-cloud hierarchical Federated Learning system, supported with a HierFAVG algorithm that allows multiple edge servers to perform partial model aggregation. In this way, the model can be trained faster and better communication-computation trade-offs can be achieved. Convergence analysis is provided for HierFAVG and the effects of key parameters are also investigated, which lead to qualitative design guidelines. Empirical experiments verify the analysis and demonstrate the benefits of this hierarchical architecture in different data distribution scenarios. Particularly, it is shown that by introducing the intermediate edge servers, the model training time and the energy consumption of the end devices can be simultaneously reduced compared to cloud-based Federated Learning.},
	urldate = {2025-01-02},
	booktitle = {{ICC} 2020 - 2020 {IEEE} {International} {Conference} on {Communications} ({ICC})},
	author = {Liu, Lumin and Zhang, Jun and Song, S.H. and Letaief, Khaled B.},
	month = jun,
	year = {2020},
	note = {ISSN: 1938-1883},
	keywords = {Cloud computing, Computational modeling, Convergence, Data models, Edge Learning, Federated Learning, Machine learning, Mobile Edge Computing, Servers, Training},
	pages = {1--6},
	file = {已提交版本:files/249/Liu 等 - 2020 - Client-Edge-Cloud Hierarchical Federated Learning.pdf:application/pdf},
}

@inproceedings{luo_no_2021,
	title = {No {Fear} of {Heterogeneity}: {Classifier} {Calibration} for {Federated} {Learning} with {Non}-{IID} {Data}},
	volume = {34},
	shorttitle = {No {Fear} of {Heterogeneity}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/2f2b265625d76a6704b08093c652fd79-Abstract.html},
	abstract = {A central challenge in training classification models in the real-world federated system is learning with non-IID data. To cope with this, most of the existing works involve enforcing regularization in local optimization or improving the model aggregation scheme at the server. Other works also share public datasets or synthesized samples to supplement the training of under-represented classes or introduce a certain level of personalization. Though effective, they lack a deep understanding of how the data heterogeneity affects each layer of a deep classification model. In this paper, we bridge this gap by performing an experimental analysis of the representations learned by different layers. Our observations are surprising: (1) there exists a greater bias in the classifier than other layers, and (2) the classification performance can be significantly improved by post-calibrating the classifier after federated training. Motivated by the above findings, we propose a novel and simple algorithm called Classifier Calibration with Virtual Representations (CCVR), which adjusts the classifier using virtual representations sampled from an approximated gaussian mixture model. Experimental results demonstrate that CCVR achieves state-of-the-art performance on popular federated learning benchmarks including CIFAR-10, CIFAR-100, and CINIC-10. We hope that our simple yet effective method can shed some light on the future research of federated learning with non-IID data.},
	urldate = {2025-01-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Luo, Mi and Chen, Fei and Hu, Dapeng and Zhang, Yifan and Liang, Jian and Feng, Jiashi},
	year = {2021},
	pages = {5972--5984},
	file = {Full Text PDF:files/248/Luo 等 - 2021 - No Fear of Heterogeneity Classifier Calibration f.pdf:application/pdf},
}

@inproceedings{zhang_upload-efficient_2024,
	title = {An {Upload}-{Efficient} {Scheme} for {Transferring} {Knowledge} {From} a {Server}-{Side} {Pre}-trained {Generator} to {Clients} in {Heterogeneous} {Federated} {Learning}},
	url = {https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_An_Upload-Efficient_Scheme_for_Transferring_Knowledge_From_a_Server-Side_Pre-trained_CVPR_2024_paper.html},
	language = {en},
	urldate = {2025-01-02},
	author = {Zhang, Jianqing and Liu, Yang and Hua, Yang and Cao, Jian},
	year = {2024},
	pages = {12109--12119},
	file = {Full Text PDF:files/254/Zhang 等 - 2024 - An Upload-Efficient Scheme for Transferring Knowle.pdf:application/pdf},
}

@inproceedings{chen_eefl_2023,
	address = {New York, NY, USA},
	series = {{MobiSys} '23},
	title = {{EEFL}: {High}-{Speed} {Wireless} {Communications} {Inspired} {Energy} {Efficient} {Federated} {Learning} over {Mobile} {Devices}},
	isbn = {9798400701108},
	shorttitle = {{EEFL}},
	url = {https://dl.acm.org/doi/10.1145/3581791.3596865},
	doi = {10.1145/3581791.3596865},
	abstract = {Energy efficiency is essential for federated learning (FL) over mobile devices and its potential prosperous applications. Different from existing communication efficient FL research efforts, which regard communication energy consumption as the bottleneck, we have observed that with ever increasing wireless transmission speed (e.g., Wi-Fi 5 or 5G), the energy consumption of wireless communications for model updates in FL is significantly reduced and sometimes is smaller than that of local on-device training. Motivated by such observations, in this paper, we propose a high-speed wireless communications inspired energy efficient federated learning over mobile devices (EEFL), whose goal is to reduce the overall energy consumption (computing + communication). In particular, we design a novel energy-aware adaptive local update policy for mobile devices by jointly considering FL performance and energy saving of high-speed wireless transmissions. Furthermore, given the device's local update policy in each FL global round, we advance the dynamic voltage and frequency scaling (DVFS) strategy to minimize local training's energy consumption by keeping GPU and CPU working at appropriate frequencies without triggering thermal throttling. Extensive experimental results with various learning models, datasets, and wireless transmission environments demonstrate the proposed EEFL's superiority over the peer designs in terms of energy efficiency.},
	urldate = {2025-01-02},
	booktitle = {Proceedings of the 21st {Annual} {International} {Conference} on {Mobile} {Systems}, {Applications} and {Services}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Rui and Wan, Qiyu and Zhang, Xinyue and Qin, Xiaoqi and Hou, Yanzhao and Wang, Di and Fu, Xin and Pan, Miao},
	month = jun,
	year = {2023},
	pages = {544--556},
	file = {Full Text PDF:files/252/Chen 等 - 2023 - EEFL High-Speed Wireless Communications Inspired .pdf:application/pdf},
}

@article{luo_communication-efficient_2024,
	title = {Communication-{Efficient} {Federated} {Learning} {With} {Adaptive} {Aggregation} for {Heterogeneous} {Client}-{Edge}-{Cloud} {Network}},
	volume = {17},
	issn = {1939-1374},
	url = {https://ieeexplore.ieee.org/abstract/document/10528890},
	doi = {10.1109/TSC.2024.3399649},
	abstract = {Client-edge-cloud Federated Learning (CEC-FL) is emerging as an increasingly popular FL paradigm, alleviating the performance limitations of conventional cloud-centric Federated Learning (FL) by incorporating edge computing. However, improving training efficiency while retaining model convergence is not easy in CEC-FL. Although controlling aggregation frequency exhibits great promise in improving efficiency by reducing communication overhead, existing works still struggle to simultaneously achieve satisfactory training efficiency and model convergence performance in heterogeneous and dynamic environments. This paper proposes FedAda, a communication-efficient CEC-FL training method that aims to enhance training performance while ensuring model convergence through adaptive aggregation frequency adjustment. To this end, we theoretically analyze the model convergence under aggregation frequency control. Based on this analysis of the relationship between model convergence and aggregation frequencies, we propose an approximation algorithm to calculate aggregation frequencies, considering convergence and aligning with heterogeneous and dynamic node capabilities, ultimately achieving superior convergence accuracy and speed. Simulation results validate the effectiveness and efficiency of FedAda, demonstrating up to 4\% improvement in test accuracy, 6.8× shorter training time and 3.3× less communication overhead compared to prior solutions.},
	number = {6},
	urldate = {2025-01-02},
	journal = {IEEE Transactions on Services Computing},
	author = {Luo, Long and Zhang, Chi and Yu, Hongfang and Sun, Gang and Luo, Shouxi and Dustdar, Schahram},
	month = nov,
	year = {2024},
	note = {Conference Name: IEEE Transactions on Services Computing},
	keywords = {Adaptation models, Aggregation frequency, client-edge-cloud, communication, Computational modeling, Convergence, federated learning, Frequency control, Optimization, Servers, Training, training efficiency},
	pages = {3241--3255},
}


@inproceedings{zhu_data-free_fedgen_2021,
	title = {Data-{Free} {Knowledge} {Distillation} for {Heterogeneous} {Federated} {Learning}},
	url = {https://proceedings.mlr.press/v139/zhu21b.html},
	abstract = {Federated Learning (FL) is a decentralized machine-learning paradigm, in which a global server iteratively averages the model parameters of local users without accessing their data. User heterogeneity has imposed significant challenges to FL, which can incur drifted global models that are slow to converge. Knowledge Distillation has recently emerged to tackle this issue, by refining the server model using aggregated knowledge from heterogeneous users, other than directly averaging their model parameters. This approach, however, depends on a proxy dataset, making it impractical unless such a prerequisite is satisfied. Moreover, the ensemble knowledge is not fully utilized to guide local model learning, which may in turn affect the quality of the aggregated model. Inspired by the prior art, we propose a data-free knowledge distillation approach to address heterogeneous FL, where the server learns a lightweight generator to ensemble user information in a data-free manner, which is then broadcasted to users, regulating local training using the learned knowledge as an inductive bias. Empirical studies powered by theoretical implications show that our approach facilitates FL with better generalization performance using fewer communication rounds, compared with the state-of-the-art.},
	language = {en},
	urldate = {2025-01-02},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhu, Zhuangdi and Hong, Junyuan and Zhou, Jiayu},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {12878--12889},
	file = {Full Text PDF:files/256/Zhu 等 - 2021 - Data-Free Knowledge Distillation for Heterogeneous.pdf:application/pdf},
}

@inproceedings{yi_fedgh_2023,
	address = {New York, NY, USA},
	series = {{MM} '23},
	title = {{FedGH}: {Heterogeneous} {Federated} {Learning} with {Generalized} {Global} {Header}},
	isbn = {9798400701085},
	shorttitle = {{FedGH}},
	url = {https://doi.org/10.1145/3581783.3611781},
	doi = {10.1145/3581783.3611781},
	abstract = {Federated learning (FL) is an emerging machine learning paradigm that allows multiple parties to train a shared model collaboratively in a privacy-preserving manner. Existing horizontal FL methods generally assume that the FL server and clients hold the same model structure. However, due to system heterogeneity and the need for personalization, enabling clients to hold models with diverse structures has become an important direction. Existing model-heterogeneous FL approaches often require publicly available datasets and incur high communication and/or computational costs, which limit their performances. To address these limitations, we propose a simple but effective Federated Global prediction Header (FedGH) approach. It is a communication and computation-efficient model-heterogeneous FL framework which trains a shared generalized global prediction header with representations extracted by heterogeneous extractors for clients' models at the FL server. The trained generalized global prediction header learns from different clients. The acquired global knowledge is then transferred to clients to substitute each client's local prediction header. We derive the non-convex convergence rate of FedGH. Extensive experiments on two real-world datasets demonstrate that FedGH achieves significantly more advantageous performance in both model-homogeneous and -heterogeneous FL scenarios compared to seven state-of-the-art personalized FL models, beating the best-performing baseline by up to 8.87\% (for model-homogeneous FL) and 1.83\% (for model-heterogeneous FL) in terms of average test accuracy, while saving up to 85.53\% of communication overhead.},
	urldate = {2025-01-02},
	booktitle = {Proceedings of the 31st {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Yi, Liping and Wang, Gang and Liu, Xiaoguang and Shi, Zhuan and Yu, Han},
	month = oct,
	year = {2023},
	pages = {8686--8696},
	file = {已提交版本:files/260/Yi 等 - 2023 - FedGH Heterogeneous Federated Learning with Gener.pdf:application/pdf},
}

@inproceedings{nguyen_federated_2022,
	title = {Federated {Learning} with {Buffered} {Asynchronous} {Aggregation}},
	url = {https://proceedings.mlr.press/v151/nguyen22b.html},
	abstract = {Scalability and privacy are two critical concerns for cross-device federated learning (FL) systems. In this work, we identify that synchronous FL – cannot scale efficiently beyond a few hundred clients training in parallel. It leads to diminishing returns in model performance and training speed, analogous to large-batch training. On the other hand, asynchronous aggregation of client updates in FL (i.e., asynchronous FL) alleviates the scalability issue. However, aggregating individual client updates is incompatible with Secure Aggregation, which could result in an undesirable level of privacy for the system. To address these concerns, we propose a novel buffered asynchronous aggregation method, FedBuff, that is agnostic to the choice of optimizer, and combines the best properties of synchronous and asynchronous FL. We empirically demonstrate that FedBuff is 3.3×3.3×3.3{\textbackslash}times more efficient than synchronous FL and up to 2.5×2.5×2.5{\textbackslash}times more efficient than asynchronous FL, while being compatible with privacy-preserving technologies such as Secure Aggregation and differential privacy. We provide theoretical convergence guarantees in a smooth non-convex setting. Finally, we show that under differentially private training, FedBuff can outperform FedAvgM at low privacy settings and achieve the same utility for higher privacy settings.},
	language = {en},
	urldate = {2025-01-02},
	booktitle = {Proceedings of {The} 25th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Nguyen, John and Malik, Kshitiz and Zhan, Hongyuan and Yousefpour, Ashkan and Rabbat, Mike and Malek, Mani and Huba, Dzmitry},
	month = may,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {3581--3607},
	file = {Full Text PDF:files/262/Nguyen 等 - 2022 - Federated Learning with Buffered Asynchronous Aggr.pdf:application/pdf},
}

@article{zhang2023pfllib,
  title={PFLlib: Personalized Federated Learning Algorithm Library},
  author={Zhang, Jianqing and Liu, Yang and Hua, Yang and Wang, Hao and Song, Tao and Xue, Zhengui and Ma, Ruhui and Cao, Jian},
  journal={arXiv preprint arXiv:2312.04992},
  year={2023}
}

@inproceedings{li2021mode_moon,
  title={Model-contrastive federated learning},
  author={Li, Qinbin and He, Bingsheng and Song, Dawn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10713--10722},
  year={2021}
}

@article{mcinnes2018umap-software,
  title={UMAP: Uniform Manifold Approximation and Projection},
  author={McInnes, Leland and Healy, John and Saul, Nathaniel and Grossberger, Lukas},
  journal={The Journal of Open Source Software},
  volume={3},
  number={29},
  pages={861},
  year={2018}
}

@article{jiang2020energy,
  title={Energy aware edge computing: A survey},
  author={Jiang, Congfeng and Fan, Tiantian and Gao, Honghao and Shi, Weisong and Liu, Liangkai and C{\'e}rin, Christophe and Wan, Jian},
  journal={Computer Communications},
  volume={151},
  pages={556--580},
  year={2020},
  publisher={Elsevier}
}

@article{shannon1948mathematical,
  title={A mathematical theory of communication},
  author={Shannon, Claude Elwood},
  journal={The Bell system technical journal},
  volume={27},
  number={3},
  pages={379--423},
  year={1948},
  publisher={Nokia Bell Labs}
}

@article{feng2021min,
  title={Min-max cost optimization for efficient hierarchical federated learning in wireless edge networks},
  author={Feng, Jie and Liu, Lei and Pei, Qingqi and Li, Keqin},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={33},
  number={11},
  pages={2687--2700},
  year={2021},
  publisher={IEEE}
}

@article{burd1996processor,
  title={Processor design for portable systems},
  author={Burd, Thomas D and Brodersen, Robert W},
  journal={Journal of VLSI signal processing systems for signal, image and video technology},
  volume={13},
  number={2},
  pages={203--221},
  year={1996},
  publisher={Springer}
}


@article{zhu2019energy,
  title={Energy minimization for multicore platforms through DVFS and VR phase scaling with comprehensive convex model},
  author={Zhu, Zuomin and Zhang, Wei and Chaturvedi, Vivek and Singh, Amit Kumar},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume={39},
  number={3},
  pages={686--699},
  year={2019},
  publisher={IEEE}
}


@article{zeb2024towards_industry5.0,
  title={Towards defining industry 5.0 vision with intelligent and softwarized wireless network architectures and services: A survey},
  author={Zeb, Shah and Mahmood, Aamir and Khowaja, Sunder Ali and Dev, Kapal and Hassan, Syed Ali and Gidlund, Mikael and Bellavista, Paolo},
  journal={Journal of Network and Computer applications},
  volume={223},
  pages={103796},
  year={2024},
  publisher={Elsevier}
}

@article{leng2024unlocking_industry5.0,
  title={Unlocking the power of industrial artificial intelligence towards Industry 5.0: Insights, pathways, and challenges},
  author={Leng, Jiewu and Zhu, Xiaofeng and Huang, Zhiqiang and Li, Xingyu and Zheng, Pai and Zhou, Xueliang and Mourtzis, Dimitris and Wang, Baicun and Qi, Qinglin and Shao, Haidong and others},
  journal={Journal of Manufacturing Systems},
  volume={73},
  pages={349--363},
  year={2024},
  publisher={Elsevier}
}

@ARTICLE{10440434,
  author={Hu, Yujiao and Jia, Qingmin and Yao, Yuan and Lee, Yong and Lee, Mengjie and Wang, Chenyi and Zhou, Xiaomao and Xie, Renchao and Yu, F. Richard},
  journal={IEEE Internet of Things Journal}, 
  title={Industrial Internet of Things Intelligence Empowering Smart Manufacturing: A Literature Review}, 
  year={2024},
  volume={11},
  number={11},
  pages={19143-19167},
  keywords={Industrial Internet of Things;Manufacturing;Computer architecture;Industries;Smart manufacturing;Market research;Costs;Artificial intelligence (AI);Industrial Internet of Things (IIoT);IIoT intelligence;smart manufacturing},
  doi={10.1109/JIOT.2024.3367692}}


@article{boobalan2022fusion,
  title={Fusion of federated learning and industrial Internet of Things: A survey},
  author={Boobalan, Parimala and Ramu, Swarna Priya and Pham, Quoc-Viet and Dev, Kapal and Pandya, Sharnil and Maddikunta, Praveen Kumar Reddy and Gadekallu, Thippa Reddy and Huynh-The, Thien},
  journal={Computer Networks},
  volume={212},
  pages={109048},
  year={2022},
  publisher={Elsevier}
}

@article{liu2024federated_sensors,
  title={Federated Learning-Oriented Edge Computing Framework for the IIoT},
  author={Liu, Xianhui and Dong, Xianghu and Jia, Ning and Zhao, Weidong},
  journal={Sensors},
  volume={24},
  number={13},
  pages={4182},
  year={2024},
  publisher={MDPI}
}

@article{de2024spatio_agriculture,
  title={Spatio-temporal semantic data management systems for IoT in agriculture 5.0: Challenges and future directions},
  author={de la Parte, Mario San Emeterio and Mart{\'\i}nez-Ortega, Jos{\'e}-Fern{\'a}n and Castillejo, Pedro and Lucas-Mart{\'\i}nez, N{\'e}stor},
  journal={Internet of Things},
  volume={25},
  pages={101030},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{fedavg_google,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@article{salim2024fl_securityguard,
  title={FL-CTIF: A federated learning based CTI framework based on information fusion for secure IIoT},
  author={Salim, Mikail Mohammed and El Azzaoui, Abir and Deng, Xianjun and Park, Jong Hyuk},
  journal={Information Fusion},
  volume={102},
  pages={102074},
  year={2024},
  publisher={Elsevier}
}

@article{rashid2023federated_invasion,
  title={A federated learning-based approach for improving intrusion detection in industrial internet of things networks},
  author={Rashid, Md Mamunur and Khan, Shahriar Usman and Eusufzai, Fariha and Redwan, Md Azharuddin and Sabuj, Saifur Rahman and Elsharief, Mahmoud},
  journal={Network},
  volume={3},
  number={1},
  pages={158--179},
  year={2023},
  publisher={MDPI}
}


@article{al2023decentralized_D2D,
  title={Decentralized aggregation for energy-efficient federated learning via D2D communications},
  author={Al-Abiad, Mohammed S and Obeed, Mohanad and Hossain, Md Jahangir and Chaaban, Anas},
  journal={IEEE Transactions on Communications},
  volume={71},
  number={6},
  pages={3333--3351},
  year={2023},
  publisher={IEEE}
}


@article{zhou2023hierarchical,
  title={Hierarchical federated learning with social context clustering-based participant selection for internet of medical things applications},
  author={Zhou, Xiaokang and Ye, Xiaozhou and Kevin, I and Wang, Kai and Liang, Wei and Nair, Nirmal Kumar C and Shimizu, Shohei and Yan, Zheng and Jin, Qun},
  journal={IEEE Transactions on Computational Social Systems},
  volume={10},
  number={4},
  pages={1742--1751},
  year={2023},
  publisher={IEEE}
}

@article{abdellatif2022communication_hierarchical,
  title={Communication-efficient hierarchical federated learning for IoT heterogeneous systems with imbalanced data},
  author={Abdellatif, Alaa Awad and Mhaisen, Naram and Mohamed, Amr and Erbad, Aiman and Guizani, Mohsen and Dawy, Zaher and Nasreddine, Wassim},
  journal={Future Generation Computer Systems},
  volume={128},
  pages={406--419},
  year={2022},
  publisher={Elsevier}
}

@article{kalra2023decentralized,
  title={Decentralized federated learning through proxy model sharing},
  author={Kalra, Shivam and Wen, Junfeng and Cresswell, Jesse C and Volkovs, Maksims and Tizhoosh, Hamid R},
  journal={Nature communications},
  volume={14},
  number={1},
  pages={2899},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{fang2024byzantine,
  title={Byzantine-robust decentralized federated learning},
  author={Fang, Minghong and Zhang, Zifan and Hairi and Khanduri, Prashant and Liu, Jia and Lu, Songtao and Liu, Yuchen and Gong, Neil},
  booktitle={Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
  pages={2874--2888},
  year={2024}
}


@inproceedings{zhu2021data,
  title={Data-free knowledge distillation for heterogeneous federated learning},
  author={Zhu, Zhuangdi and Hong, Junyuan and Zhou, Jiayu},
  booktitle={International conference on machine learning},
  pages={12878--12889},
  year={2021},
  organization={PMLR}
}
@article{jeong2018communication,
  title={Communication-efficient on-device machine learning: Federated distillation and augmentation under non-iid private data},
  author={Jeong, Eunjeong and Oh, Seungeun and Kim, Hyesung and Park, Jihong and Bennis, Mehdi and Kim, Seong-Lyun},
  journal={arXiv preprint arXiv:1811.11479},
  year={2018}
}

@article{wu2022communication,
  title={Communication-efficient federated learning via knowledge distillation},
  author={Wu, Chuhan and Wu, Fangzhao and Lyu, Lingjuan and Huang, Yongfeng and Xie, Xing},
  journal={Nature communications},
  volume={13},
  number={1},
  pages={2032},
  year={2022},
  publisher={Nature Publishing Group UK London}
}


@inproceedings{you2023zeus,
  title={Zeus: Understanding and optimizing $\{$GPU$\}$ energy consumption of $\{$DNN$\}$ training},
  author={You, Jie and Chung, Jae-Won and Chowdhury, Mosharaf},
  booktitle={20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)},
  pages={119--139},
  year={2023}
}


@article{dai2024energy,
  title={Energy-Efficient Inference With Software-Hardware Co-Design for Sustainable Artificial Intelligence of Things},
  author={Dai, Shengxin and Luo, Zheng and Luo, Wendian and Wang, Siyi and Dai, Cheng and Guo, Bing and Zhou, Xiaokang},
  journal={IEEE Internet of Things Journal},
  year={2024},
  publisher={IEEE}
}



@inproceedings{guo2022bofl_DVFS,
  title={BoFL: bayesian optimized local training pace control for energy efficient federated learning},
  author={Guo, Hongpeng and Gu, Haotian and Yang, Zhe and Wang, Xiaoyang and Lee, Eun Kyung and Chandramoorthy, Nandhini and Eilam, Tamar and Chen, Deming and Nahrstedt, Klara},
  booktitle={Proceedings of the 23rd ACM/IFIP International Middleware Conference},
  pages={188--201},
  year={2022}
}


@article{de2023hed_quantize,
  title={HED-FL: A hierarchical, energy efficient, and dynamic approach for edge Federated Learning},
  author={De Rango, Floriano and Guerrieri, Antonio and Raimondo, Pierfrancesco and Spezzano, Giandomenico},
  journal={Pervasive and Mobile Computing},
  volume={92},
  pages={101804},
  year={2023},
  publisher={Elsevier}
}

@article{chen2022energy_quantize,
  title={Energy efficient federated learning over heterogeneous mobile devices via joint design of weight quantization and wireless transmission},
  author={Chen, Rui and Li, Liang and Xue, Kaiping and Zhang, Chi and Pan, Miao and Fang, Yuguang},
  journal={IEEE Transactions on Mobile Computing},
  volume={22},
  number={12},
  pages={7451--7465},
  year={2022},
  publisher={IEEE}
}

@article{luo2024communication_CEC,
  title={Communication-Efficient Federated Learning with Adaptive Aggregation for Heterogeneous Client-Edge-Cloud Network},
  author={Luo, Long and Zhang, Chi and Yu, Hongfang and Sun, Gang and Luo, Shouxi and Dustdar, Schahram},
  journal={IEEE Transactions on Services Computing},
  year={2024},
  publisher={IEEE}
}

@article{wu2023hiflash_CEC,
  title={HiFlash: Communication-efficient hierarchical federated learning with adaptive staleness control and heterogeneity-aware client-edge association},
  author={Wu, Qiong and Chen, Xu and Ouyang, Tao and Zhou, Zhi and Zhang, Xiaoxi and Yang, Shusen and Zhang, Junshan},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={34},
  number={5},
  pages={1560--1579},
  year={2023},
  publisher={IEEE}
}